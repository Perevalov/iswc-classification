{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\n\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\nimport gc\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.callbacks import Callback \nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helper Functions"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(transformer, max_len=512, hidden_dim=32, num_of_classes=3):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    \n    out = Dense(num_of_classes, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TPU Configs"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 96\nMODEL = 'bert-base-multilingual-cased'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Create fast tokenizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# First load the real tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load text data into memory"},{"metadata":{"trusted":true},"cell_type":"code","source":"dbpedia_df = pd.read_csv(\"/kaggle/input/test-06366/dbpedia.csv\", sep=\"|\")\ndbpedia_df.type_1.fillna(value=\"NoneType\")\ndbpedia_df.type_1 = dbpedia_df.type_1.astype(str)\n\nwikidata_df = pd.read_csv(\"/kaggle/input/test-06366/wikidata.csv\", sep=\"|\")\nwikidata_df.type_1.fillna(value=\"NoneType\")\nwikidata_df.type_1 = wikidata_df.type_1.astype(str)\n\ncommon_df = dbpedia_df.append(wikidata_df)\nliteral_df = common_df[common_df.category == \"literal\"]\n\nX_cat_train, X_cat_test, y_cat_train, y_cat_test = train_test_split(common_df.question.astype(str), common_df.category, test_size=0.33, random_state=42)\nX_lit_train, X_lit_test, y_lit_train, y_lit_test = train_test_split(literal_df.question.astype(str), literal_df.type_1, test_size=0.33, random_state=42)\nX_db_train, X_db_test, y_db_train, y_db_test = train_test_split(dbpedia_df[dbpedia_df.category == \"resource\"].question.astype(str), dbpedia_df[dbpedia_df.category == \"resource\"].type_1, test_size=0.33, random_state=42)\nX_wiki_train, X_wiki_test, y_wiki_train, y_wiki_test = train_test_split(wikidata_df[wikidata_df.category == \"resource\"].question.astype(str), wikidata_df[wikidata_df.category == \"resource\"].type_1, test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Category classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_cat = LabelEncoder()\nencoder_cat.fit(common_df.category)\nencoded_Y_cat_train = encoder_cat.transform(y_cat_train)\nencoded_Y_cat_test = encoder_cat.transform(y_cat_test)\ndummy_y_cat_train = np_utils.to_categorical(encoded_Y_cat_train) # convert integers to dummy variables (i.e. one hot encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nx_cat_train = regular_encode(X_cat_train.values.astype(str), tokenizer, maxlen=MAX_LEN)\nx_cat_test = regular_encode(X_cat_test.values.astype(str), tokenizer, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset_cat = (\n    tf.data.Dataset\n    .from_tensor_slices((x_cat_train, dummy_y_cat_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\ntest_dataset_cat = (\n    tf.data.Dataset\n    .from_tensor_slices(x_cat_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model_cat = build_model(transformer_layer, max_len=MAX_LEN, num_of_classes=common_df.category.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    verbose=1,\n    patience=1,\n    mode='min',\n    restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_cat_train.shape[0] // BATCH_SIZE\ntrain_history = model_cat.fit(\n    train_dataset_cat,\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS,\n    callbacks=[early_stopping]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cat_test_pred = np.argmax(model_cat.predict(test_dataset_cat, verbose=1), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = accuracy_score(encoded_Y_cat_test, y_cat_test_pred)\nprint('Accuracy: {0}'.format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Literal classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_lit = LabelEncoder()\nencoder_lit.fit(literal_df.type_1)\nencoded_Y_lit_train = encoder_lit.transform(y_lit_train)\nencoded_Y_lit_test = encoder_lit.transform(y_lit_test)\ndummy_y_lit_train = np_utils.to_categorical(encoded_Y_lit_train) # convert integers to dummy variables (i.e. one hot encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nx_lit_train = regular_encode(X_lit_train.values.astype(str), tokenizer, maxlen=MAX_LEN)\nx_lit_test = regular_encode(X_lit_test.values.astype(str), tokenizer, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset_lit = (\n    tf.data.Dataset\n    .from_tensor_slices((x_lit_train, dummy_y_lit_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\ntest_dataset_lit = (\n    tf.data.Dataset\n    .from_tensor_slices(x_lit_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model_lit = build_model(transformer_layer, max_len=MAX_LEN, num_of_classes=literal_df.type_1.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_lit_train.shape[0] // BATCH_SIZE\ntrain_history = model_lit.fit(\n    train_dataset_lit,\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS,\n    callbacks=[early_stopping]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_lit_test_pred = np.argmax(model_lit.predict(test_dataset_lit, verbose=1), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = accuracy_score(encoded_Y_lit_test, y_lit_test_pred)\nprint('Accuracy: {0}'.format(acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## DBpedia Resource classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_db = LabelEncoder()\nencoder_db.fit(dbpedia_df[dbpedia_df.category == \"resource\"].type_1)\nencoded_Y_db_train = encoder_db.transform(y_db_train)\nencoded_Y_db_test = encoder_db.transform(y_db_test)\ndummy_y_db_train = np_utils.to_categorical(encoded_Y_db_train) # convert integers to dummy variables (i.e. one hot encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nx_db_train = regular_encode(X_db_train.values.astype(str), tokenizer, maxlen=MAX_LEN)\nx_db_test = regular_encode(X_db_test.values.astype(str), tokenizer, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset_db = (\n    tf.data.Dataset\n    .from_tensor_slices((x_db_train, dummy_y_db_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\ntest_dataset_db = (\n    tf.data.Dataset\n    .from_tensor_slices(x_db_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model_db = build_model(transformer_layer, max_len=MAX_LEN, num_of_classes=dbpedia_df[dbpedia_df.category == \"resource\"].type_1.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_db_train.shape[0] // BATCH_SIZE\ntrain_history = model_db.fit(\n    train_dataset_db,\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS,\n    callbacks=[early_stopping]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_db_test_pred = np.argmax(model_db.predict(test_dataset_db, verbose=1), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = f1_score(encoded_Y_db_test, y_db_test_pred, average='weighted')\nprint('Accuracy: {0}'.format(f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Wikidata Resource classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_wiki = LabelEncoder()\nencoder_wiki.fit(wikidata_df[wikidata_df.category == \"resource\"].type_1)\nencoded_Y_wiki_train = encoder_wiki.transform(y_wiki_train)\nencoded_Y_wiki_test = encoder_wiki.transform(y_wiki_test)\ndummy_y_wiki_train = np_utils.to_categorical(encoded_Y_wiki_train) # convert integers to dummy variables (i.e. one hot encoded)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n\nx_wiki_train = regular_encode(X_wiki_train.values.astype(str), tokenizer, maxlen=MAX_LEN)\nx_wiki_test = regular_encode(X_wiki_test.values.astype(str), tokenizer, maxlen=MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset_wiki = (\n    tf.data.Dataset\n    .from_tensor_slices((x_wiki_train, dummy_y_wiki_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\ntest_dataset_wiki = (\n    tf.data.Dataset\n    .from_tensor_slices(x_wiki_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model_wiki = build_model(transformer_layer, max_len=MAX_LEN, num_of_classes=wikidata_df[wikidata_df.category == \"resource\"].type_1.nunique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_steps = x_wiki_train.shape[0] // BATCH_SIZE\ntrain_history = model_wiki.fit(\n    train_dataset_wiki,\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS,\n    callbacks=[early_stopping]\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_wiki_test_pred = np.argmax(model_wiki.predict(test_dataset_wiki, verbose=1), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f1 = f1_score(encoded_Y_wiki_test, y_wiki_test_pred, average='weighted')\nprint('Accuracy: {0}'.format(f1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_args(M, n=10):\n    return M.argsort(axis=1)[:,::-1][:,:n]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submission preparation (for DBpedia)\n\n```output_example = [\n    {\n        \"id\" : \"\",\n        \"category\" : \"\",\n        \"type\" : []\n    },\n]```"},{"metadata":{"trusted":true},"cell_type":"code","source":"dbpedia_faketest_df = dbpedia_df[:150]\n\nx_fake_test = regular_encode(dbpedia_faketest_df.question.values.astype(str), tokenizer, maxlen=MAX_LEN)\n\nfaketest_dataset_db = (\n    tf.data.Dataset\n    .from_tensor_slices(x_fake_test)\n    .batch(BATCH_SIZE)\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting categories\ny_cat_test_pred = np.argmax(model_cat.predict(faketest_dataset_db, verbose=1), axis=1)\ny_pred_cat_labels = encoder_cat.inverse_transform(y_cat_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting literals (not all of them will be used)\ny_lit_test_pred = np.argmax(model_lit.predict(faketest_dataset_db, verbose=1), axis=1)\ny_pred_lit_labels = encoder_lit.inverse_transform(y_lit_test_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#predicting resources (not all of them will be used)\ny_res_test_pred = get_top_n_args(model_db.predict(faketest_dataset_db, verbose=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_array = list()\n\nfor i in range(len(y_pred_cat_labels)):\n    pred = {\"id\": dbpedia_faketest_df.iloc[i][\"id\"], \"category\": y_pred_cat_labels[i], \"type\": []}\n    \n    if y_pred_cat_labels[i] == \"boolean\":\n        pred[\"type\"] = [y_pred_cat_labels[i]]\n        preds_array.append(pred)\n    elif y_pred_cat_labels[i] == \"literal\":\n        pred[\"type\"] = [y_pred_lit_labels[i]]\n        preds_array.append(pred)\n    elif y_pred_cat_labels[i] == \"resource\":\n        res_arr = encoder_db.inverse_transform(y_res_test_pred[i])\n        pred[\"type\"] = list(res_arr)\n        preds_array.append(pred)\n    else:\n        raise AssertionError()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}